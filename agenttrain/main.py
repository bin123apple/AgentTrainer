from datasets import Dataset, load_from_disk
from trl import GRPOConfig, ModelConfig, ScriptArguments, TrlParser, get_peft_config
import torch
import io
import re
import ast
from agenttrain.vlm_modules import *
from tools import crop
from pathlib import Path
from utils import preprocess_dataset, get_model_and_tokenizer
from envs.tool_env import ToolEnv
from PIL import Image, ImageDraw
from collections import defaultdict
from trainers.grpo_env_trainer import GRPOEnvTrainer
from agenttrain.prompts.system_prompts import CROP_SYSTEM_PROMPT
from transformers import AutoModelForCausalLM, Qwen2_5_VLForConditionalGeneration, TrainerCallback
from deepspeed.runtime.zero.partition_parameters import GatheredParameters 
"""
Multi-GPU training (single node, 4 training + 4 inference)

CUDA_VISIBLE_DEVICES=0,1,2,3 python agenttrain/inference/vllm_serve.py --model 'Qwen/Qwen2.5-7B-Instruct' \
    --tensor_parallel_size 4 --max_model_len 8192 --dtype bfloat16 \
    --gpu_memory_utilization 0.9 --enable_prefix_caching True \
    --host 0.0.0.0 --port 8000

CUDA_VISIBLE_DEVICES=4,5,6,7 accelerate launch --config-file configs/zero3.yaml agenttrain/examples/math_train.py
"""
def load_processed_dataset(data_path: str) -> Dataset:
    """
    åŠ è½½é¢„å¤„ç†å¥½çš„æ•°æ®é›†
    
    Args:
        data_path: é¢„å¤„ç†æ•°æ®çš„è·¯å¾„
    
    Returns:
        Dataset: åŠ è½½çš„æ•°æ®é›†
    """
    print(f"ä» {data_path} åŠ è½½é¢„å¤„ç†æ•°æ®...")
    dataset = load_from_disk(data_path)
    print(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå¤§å°: {len(dataset)}")
    return dataset

def get_vlm_module(model_name_or_path):
    if "qwen" in model_name_or_path.lower():
        return Qwen2VLModule
    elif "internvl" in model_name_or_path.lower():
        return InvernVLModule
    else:
        raise ValueError(f"Unsupported model: {model_name_or_path}")

class GRPOModelConfig(ModelConfig):
    freeze_vision_modules: bool = False
    
class RefModelRefreshCallback(TrainerCallback):
    def __init__(self, refresh_fn, refresh_interval=1000):
        self.refresh_fn = refresh_fn
        self.refresh_interval = refresh_interval

    def on_step_end(self, args, state, control, **kwargs):
        # è¿™é‡Œè§¦å‘ç‚¹åœ¨ optimizer.step() ä¹‹åï¼Œæ¢¯åº¦å·²æ¸…é›¶ï¼Œæœ€å®‰å…¨
        if state.global_step % self.refresh_interval == 0 and state.global_step > 0:
            self.refresh_fn()

class DebugGRPOCallback(TrainerCallback):
    def __init__(self, topk: int = 5):
        self.topk = topk
        # ä¿å­˜ä¸Šä¸€æ­¥å‚æ•°å‡å€¼ï¼Œé»˜è®¤ 0
        self.last_means = defaultdict(float)

    def on_pre_optimizer_step(self, args, state, control, model=None, **kwargs):
        if not state.is_local_process_zero:
            return  # åªåœ¨ rank 0 æ‰“å°ä¸€æ¬¡
        print(f"\n>>> ğŸ”” on_pre_optimizer_step (step {state.global_step})")
        for name, p in model.named_parameters():
            # èšåˆå‚æ•°åˆ†ç‰‡ï¼ˆåŒæ ·ä¹Ÿä¼šæŠŠå¯¹åº”çš„ grad shard æ”¶é½åˆ° rank0ï¼‰
            with GatheredParameters([p], modifier_rank=0):
                # è¿™é‡Œ p.grad å°±æ˜¯å®Œæ•´æ¢¯åº¦äº†
                full_grad = p.grad.clone().cpu() if p.grad is not None else None

            if full_grad is None:
                grad_norm = 0.0
            else:
                grad_norm = full_grad.norm().item()
            if grad_norm > 0:
                print(f"{name:60s} | grad_norm={grad_norm:.6e}")

    def on_optimizer_step(self, args, state, control, model=None, optimizer=None, **kwargs):
        # å‚æ•°åˆšæ›´æ–°ï¼Œæ‰“å°æœ‰æ›´æ–°çš„å±‚
        print(f"\n>>> ğŸ”” on_optimizer_step (step {state.global_step})")
        updated = []
        for name, p in list(model.named_parameters()):
            # å¯¹ DeepSpeed ZeRO-3 åšèšåˆ
            with GatheredParameters([p], modifier_rank=0):
                full_p = p.clone().cpu()
            mean = full_p.mean().item()
            delta = mean - self.last_means[name]
            if delta != 0:
                updated.append((name, mean, delta))
            # æ›´æ–° last_means
            self.last_means[name] = mean

        if updated:
            print("Updated layers:")
            for name, mean, delta in updated:
                print(f"{name:60s} | mean={mean:.12f} Î”mean={delta:.12f}")
        else:
            print("No layers updated in this step.")
            
def main():
    """ä¸»å‡½æ•°"""
    torch._dynamo.config.disable = True
    
    # 1. åŠ è½½é¢„å¤„ç†æ•°æ®
    try:
        PROCESSED_DATA_PATH = "/mnt/data1/processed_datasets/uground_processed_10000_20000"
        dataset = load_processed_dataset(PROCESSED_DATA_PATH)
    except Exception as e:
        print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
        print("è¯·å…ˆè¿è¡Œ agenttrain/utils/data_collection_save.py ç”Ÿæˆé¢„å¤„ç†æ•°æ®")
        return  # æˆ–è€… raise e æ¥åœæ­¢ç¨‹åº
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœé¢„å¤„ç†æ•°æ®ä¸å­˜åœ¨ï¼Œå¯ä»¥ä¸´æ—¶ä½¿ç”¨åŸå§‹å¤„ç†æ–¹å¼
        # from your_preprocess_module import preprocess_dataset
        # print("å›é€€åˆ°åŸå§‹é¢„å¤„ç†æ–¹å¼...")
        # dataset = preprocess_dataset(
        #     "osunlp/UGround-V1-Data", 
        #     "train", 
        #     n=10000,
        #     cache_dir="/mnt/data1/huggingface/datasets/datasets--osunlp--UGround-V1-Data-Box"
        # )
    
    # 2. éšæœºæ‰“ä¹±å¹¶æŒ‰æ¯”ä¾‹åˆ†å‰²æ•°æ®é›†
    print("2. åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†...")
    split = dataset.shuffle(seed=42).train_test_split(test_size=0.01, seed=0)
    
    train_dataset = split["train"]    # 90% ç”¨äºè®­ç»ƒ
    # print(f"Fist record in train dataset: {train_dataset[0]}")
    eval_dataset = split["test"]      # 10% ç”¨äºè¯„ä¼°
    
    # éšæœºæ‰“ä¹±ï¼Œå–å‰ 50 æ¡ï¼ˆå¦‚æœä¸è¶³ 50ï¼Œåˆ™å–å…¨éƒ¨ï¼‰
    debug_root = Path("debug")
    debug_root.mkdir(parents=True, exist_ok=True)
    subset = train_dataset.shuffle(seed=42).select(range(min(50, len(train_dataset))))

    for idx, sample in enumerate(subset):
        folder = debug_root / f"sample_{idx}"
        folder.mkdir(parents=True, exist_ok=True)
        # 1) ä¿å­˜é—®é¢˜
        q = sample.get("question", "")
        (folder / "question.txt").write_text(q, encoding="utf-8")

        # 2) åŠ è½½åŸå§‹å›¾åƒ
        img = Image.open(io.BytesIO(sample["image"])).convert("RGB")
        draw = ImageDraw.Draw(img)

        # 3) è§£æ answer ä¸­çš„ bbox
        raw = sample.get("answer", "")
        try:
            # å°è¯• Python è¯­æ³•è§£æ
            bbox = tuple(ast.literal_eval(raw))
        except Exception:
            # å›é€€åˆ°æ­£åˆ™æå–æ‰€æœ‰æ•°å­—
            nums = re.findall(r"-?\\d+", str(raw))
            bbox = tuple(map(int, nums))

        # 4) åœ¨å›¾åƒä¸Šç”»çº¢æ¡†
        draw.rectangle(bbox, outline="red", width=3)

        # 5) ä¿å­˜å¸¦æ¡†çš„å›¾åƒ
        img.save(folder / "image.png")

    print(f"âœ… å·²å°† {len(subset)} ä¸ªæ ·æœ¬ä¿å­˜åˆ° {debug_root}/ ä¸‹ï¼Œæ¯ä¸ªå­æ–‡ä»¶å¤¹åŒ…å« question.txt å’Œ image.pngã€‚")
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(eval_dataset)}")
    
    # 3. è®¾ç½®å·¥å…·ç¯å¢ƒ
    print("3. åˆå§‹åŒ–å·¥å…·ç¯å¢ƒ...")
    tool_env = ToolEnv(
        dataset=train_dataset,
        eval_dataset=eval_dataset,
        system_prompt=CROP_SYSTEM_PROMPT,
        few_shot=[],
        tools=[crop],
        max_steps=5
    )
    
    print("System Prompt:")
    print(tool_env.system_prompt)
    
    # 4. åŠ è½½æ¨¡å‹
    print("4. åŠ è½½æ¨¡å‹...")
    model_name = "/mnt/data1/home/lei00126/LLaMA-Factory/saves/qwen2_5vl_ui-tars-7b/full/sft"
    # model_name = "/mnt/data1/home/lei00126/AgentTrainer/outputs/VG-grpo_qwen2_5vl-7b-vg-sft-2633-steps/checkpoint-4400"
    # model, tokenizer = get_model_and_tokenizer(
    #     model_name, 
    #     cache_dir="/mnt/data1/huggingface/models"
    # )
    # Load the VLM module
    vlm_module_cls = get_vlm_module(model_name)
    print("using vlm module:", vlm_module_cls.__name__)
    
    # 5. è®¾ç½®è®­ç»ƒå‚æ•°
    run_name = "VG-grpo_" + model_name.split("/")[-1].lower()
    
    training_args = GRPOConfig(
        output_dir=f"outputs/{run_name}",
        run_name=run_name,
        learning_rate=1e-6,
        lr_scheduler_type="constant_with_warmup",
        warmup_steps=10,
        num_train_epochs=1,
        temperature=1.0,
        # max_steps=1000000,
        epsilon = 0.2,
        epsilon_high= 0.28,
        bf16=True,
        max_grad_norm=0.01,
        num_iterations=2,
        beta=0.02,
        max_prompt_length=1024,
        max_completion_length=4096,
        per_device_train_batch_size=6,
        per_device_eval_batch_size=6,
        num_generations=6,
        gradient_accumulation_steps=16,
        gradient_checkpointing=True,
        eval_strategy="steps",
        eval_steps=10000,
        eval_accumulation_steps=1,
        eval_on_start=False,
        save_strategy="steps",
        save_steps=400,
        save_only_model=False,
        use_vllm=True,
        vllm_server_host="0.0.0.0",  # å¤šèŠ‚ç‚¹è®¾ç½®æ—¶æ›¿æ¢ä¸ºæ¨ç†æœåŠ¡å™¨çš„ä¸»æœº
        vllm_server_port=8888,
        vllm_gpu_memory_utilization=0.9,
        logging_steps=1,
        log_on_each_node=False,
        log_completions=True,
        report_to="wandb", # wandb/none
        reward_weights=tool_env.get_reward_weights(),
        sync_ref_model = False,  # æ˜¯å¦åŒæ­¥å‚è€ƒæ¨¡å‹
        # ref_model_sync_steps = 20,
    )
    # steps(æ¢¯åº¦æ›´æ–°æ¬¡æ•°) = data_amount(æ€»è®­ç»ƒæ•°æ®é‡)*num_iterations(ç›¸å½“äºæ¯ç»„æ•°æ®ç”¨å‡ æ¬¡)*num_generations(æ¯ä¸ªæ•°æ®ç”Ÿæˆå¤šå°‘ä¸ªå›ç­”)
    # / (gradient_accumulation_steps(ç§¯ç´¯å‡ æ¬¡æ¢¯åº¦æ›´æ–°)*per_device_train_batch_size(æ¯ä¸ªGPUçš„batchå¤§å°)*num_gpus(ä½¿ç”¨çš„GPUæ•°é‡))
    # model_args = ModelConfig(
    #     use_peft = True,
    #     lora_r = 64,
    #     lora_alpha = 128,
    #     lora_dropout = 0.05,
    #     lora_task_type = "CAUSAL_LM",
    # ) # For lora

    # ä¿å­˜åŸå§‹æ–¹æ³•å¹¶åˆ›å»ºè¡¥ä¸
    _original_from_pretrained = AutoModelForCausalLM.from_pretrained

    def _vl_compatible_from_pretrained(pretrained_model_name_or_path, *args, **kwargs):
        if isinstance(pretrained_model_name_or_path, str) and ("VL" in pretrained_model_name_or_path):
            return Qwen2_5_VLForConditionalGeneration.from_pretrained(
                pretrained_model_name_or_path,
                trust_remote_code=True,
                **kwargs
            )
        return _original_from_pretrained(pretrained_model_name_or_path, *args, **kwargs)

    # åº”ç”¨è¡¥ä¸
    AutoModelForCausalLM.from_pretrained = _vl_compatible_from_pretrained
    
    # 6. åˆå§‹åŒ–è®­ç»ƒå™¨
    print("5. åˆå§‹åŒ–è®­ç»ƒå™¨...")
    trainer = GRPOEnvTrainer(
        model=model_name,
        reward_funcs=tool_env.get_reward_funcs(),
        reward_weights=tool_env.get_reward_weights(),
        env=tool_env,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        vlm_module=vlm_module_cls()
        # peft_config=get_peft_config(model_args), # For lora
    )
    
    # Add callback
    refresh_cb = RefModelRefreshCallback(
        refresh_fn=trainer._refresh_reference_model,   # ç°åœ¨ trainer å·²ç»å­˜åœ¨
        refresh_interval=30
    )
    # debug_cb = DebugGRPOCallback(topk=5)
    # trainer.add_callback(refresh_cb)
    # trainer.add_callback(debug_cb)
    
    # 7. å¼€å§‹è®­ç»ƒ
    print("6. å¼€å§‹è®­ç»ƒ...")
    # trainer.train(resume_from_checkpoint = '/mnt/data1/home/lei00126/AgentTrainer/outputs/VG-grpo_sft/checkpoint-6000')
    trainer.train()
    
    print("è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main()