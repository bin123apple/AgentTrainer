0 bash -lc 'conda activate agenttrain; cd /pscratch/sd/x/xu001536/AgentTrainer; python -m agenttrain.inference.vllm_serve   --model '"/pscratch/sd/x/xu001536/.cache/huggingface/hub/models--Bin12345--qwen2_5vl_ui-tars-7b_2561_samples_1_epoch_sft/snapshots/728ef8a34d8f448a08dec32d1e6cd9e8cb492071"'   --data_parallel_size '"4"'   --tensor_parallel_size 1   --gpu_memory_utilization 0.95   --enable_prefix_caching True   --host '"128.55.82.80"'   --port 8888   > vllm_log.log 2>&1 & sleep 5; echo "Waiting for vLLM to come up..."; until grep -q "Uvicorn running on" vllm_log.log; do sleep 2; done; echo "vLLM is up!"; sleep 5; echo "Monitoring training logs..."; until grep -q "Traceback (most recent call last):" training_nccl_test_with_system_nccl.log; do sleep 5; done; echo ">>> Training encountered an error. Check the logs for details."'

1-3 bash -lc 'conda activate agenttrain; cd /pscratch/sd/x/xu001536/AgentTrainer; echo "Waiting for vLLM to come up..."; until grep -q "Uvicorn running on" vllm_log.log; do sleep 2; done; echo "vLLM is up!"; export MASTER_ADDR="'"128.55.82.147"'"; export MASTER_PORT="'"29500"'"; export NUM_MACHINES="'"3"'"; export WORLD_SIZE="'"12"'"; accelerate launch   --config-file '"agenttrain/configs/zero3.yaml"'   --num_machines $NUM_MACHINES   --num_processes $WORLD_SIZE   --machine_rank $((SLURM_PROCID-1))   --main_process_ip $MASTER_ADDR   --main_process_port $MASTER_PORT   --rdzv_backend=c10d   --max_restarts 1   '"agenttrain/main.py"'   --vllm-server-host '"128.55.82.80"'   > training_nccl_test_with_system_nccl.log 2>&1 & sleep 5; echo "Monitoring training logs..."; until grep -q "Traceback (most recent call last):" training_nccl_test_with_system_nccl.log; do sleep 5; done; echo ">>> Training encountered an error. Check the logs for details."'
